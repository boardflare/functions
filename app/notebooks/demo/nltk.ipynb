{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "8384688e",
            "metadata": {
                "tags": [
                    "Runner"
                ]
            },
            "outputs": [],
            "source": [
                "def run_tests(func, test_cases):\n",
                "    for i, args in enumerate(test_cases):\n",
                "        result = func(*args)\n",
                "        formatted_args = [f'\"{arg}\"' if isinstance(arg, str) else str(arg) for arg in args]\n",
                "        excel_formula = f\"={func.__name__.upper()}({', '.join(formatted_args)})\"\n",
                "        print(f\"Case {i+1}: {args} -> {result} | Excel: {excel_formula}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "intent_classification_improved",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: [\"What's the temperature today?\"] -> Question | Excel: =CLASSIFY_INTENT(\"What's the temperature today?\")\n",
                        "Case 2: ['Can you help me find my files?'] -> Request | Excel: =CLASSIFY_INTENT(\"Can you help me find my files?\")\n",
                        "Case 3: ['Hi, nice to meet you!'] -> Greeting | Excel: =CLASSIFY_INTENT(\"Hi, nice to meet you!\")\n",
                        "Case 4: ['Bye, thanks for all your help!'] -> Farewell | Excel: =CLASSIFY_INTENT(\"Bye, thanks for all your help!\")\n",
                        "Case 5: ['The system is working perfectly.'] -> Question | Excel: =CLASSIFY_INTENT(\"The system is working perfectly.\")\n",
                        "Case 6: ['Where did you put the documents?'] -> Question | Excel: =CLASSIFY_INTENT(\"Where did you put the documents?\")\n",
                        "Case 7: ['Please show me the way.'] -> Request | Excel: =CLASSIFY_INTENT(\"Please show me the way.\")\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "def classify_intent(text):\n",
                "    \"\"\"Classify the intent of the input text using NLTK's NaiveBayesClassifier.\n",
                "    Args:\n",
                "        text (str): Text to analyze\n",
                "    Returns:\n",
                "        str: Classified intent (capitalized)\n",
                "    \"\"\"\n",
                "    import nltk\n",
                "    from nltk.classify import NaiveBayesClassifier\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    nltk.download('punkt')\n",
                "    \n",
                "    def extract_features(text):\n",
                "        \"\"\"Extract features from text for classification.\"\"\"\n",
                "        words = word_tokenize(text.lower())\n",
                "        return dict([(word, True) for word in words])\n",
                "    \n",
                "    # Training data with labeled intents\n",
                "    training_data = [\n",
                "        (\"what is the weather like\", \"question\"),\n",
                "        (\"what time is it\", \"question\"),\n",
                "        (\"where are you from\", \"question\"),\n",
                "        (\"who made this\", \"question\"),\n",
                "        (\"please help me\", \"request\"),\n",
                "        (\"could you assist me\", \"request\"),\n",
                "        (\"show me how to\", \"request\"),\n",
                "        (\"i need help with\", \"request\"),\n",
                "        (\"hello there\", \"greeting\"),\n",
                "        (\"hi how are you\", \"greeting\"),\n",
                "        (\"good morning\", \"greeting\"),\n",
                "        (\"nice to meet you\", \"greeting\"),\n",
                "        (\"goodbye for now\", \"farewell\"),\n",
                "        (\"see you later\", \"farewell\"),\n",
                "        (\"i have to go\", \"farewell\"),\n",
                "        (\"thanks for your help\", \"farewell\"),\n",
                "        (\"i like this product\", \"statement\"),\n",
                "        (\"the weather is nice\", \"statement\"),\n",
                "        (\"this works well\", \"statement\"),\n",
                "        (\"interesting idea\", \"statement\")\n",
                "    ]\n",
                "    \n",
                "    # Prepare and train the classifier\n",
                "    featuresets = [(extract_features(text), intent) for (text, intent) in training_data]\n",
                "    classifier = NaiveBayesClassifier.train(featuresets)\n",
                "    \n",
                "    # Classify the input text\n",
                "    features = extract_features(text)\n",
                "    intent = classifier.classify(features)\n",
                "    return intent.capitalize()\n",
                "\n",
                "test_cases = [\n",
                "    [\"What's the temperature today?\"],\n",
                "    [\"Can you help me find my files?\"],\n",
                "    [\"Hi, nice to meet you!\"],\n",
                "    [\"Bye, thanks for all your help!\"],\n",
                "    [\"The system is working perfectly.\"],\n",
                "    [\"Where did you put the documents?\"],\n",
                "    [\"Please show me the way.\"]\n",
                "]\n",
                "\n",
                "run_tests(classify_intent, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "9e8116cf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
                        "[nltk_data] Downloading package words to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package words is already up-to-date!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['John works at Microsoft in Seattle.'] -> ['John (PERSON)', 'Microsoft (ORGANIZATION)', 'Seattle (GPE)'] | Excel: =NAMED_ENTITY_RECOGNITION(\"John works at Microsoft in Seattle.\")\n",
                        "Case 2: ['The United States and Canada signed a trade agreement.'] -> ['United States (GPE)', 'Canada (GPE)'] | Excel: =NAMED_ENTITY_RECOGNITION(\"The United States and Canada signed a trade agreement.\")\n",
                        "Case 3: ['Tesla CEO Elon Musk announced new plans.'] -> ['Tesla (PERSON)', 'CEO Elon Musk (ORGANIZATION)'] | Excel: =NAMED_ENTITY_RECOGNITION(\"Tesla CEO Elon Musk announced new plans.\")\n",
                        "Case 4: ['Mount Everest is in Nepal.'] -> ['Mount (PERSON)', 'Everest (ORGANIZATION)', 'Nepal (GPE)'] | Excel: =NAMED_ENTITY_RECOGNITION(\"Mount Everest is in Nepal.\")\n",
                        "Case 5: ['Sarah visited Paris last summer.'] -> ['Sarah (PERSON)', 'Paris (GPE)'] | Excel: =NAMED_ENTITY_RECOGNITION(\"Sarah visited Paris last summer.\")\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "import numpy\n",
                "\n",
                "# Download all required NLTK data\n",
                "for package in ['punkt', 'averaged_perceptron_tagger_eng', 'maxent_ne_chunker_tab', 'words']:\n",
                "    nltk.download(package)\n",
                "\n",
                "def named_entity_recognition(text):\n",
                "    \"\"\"Extract named entities from text.\n",
                "    Args:\n",
                "        text (str): Text to analyze\n",
                "    Returns:\n",
                "        list: List of named entities found\n",
                "    \"\"\"\n",
                "    # Tokenize and tag the text\n",
                "    tokens = nltk.word_tokenize(text)\n",
                "    pos_tags = nltk.pos_tag(tokens)\n",
                "    \n",
                "    # Extract named entities\n",
                "    named_entities = nltk.ne_chunk(pos_tags)\n",
                "    entities = []\n",
                "    \n",
                "    # Process each chunk\n",
                "    for chunk in named_entities:\n",
                "        if hasattr(chunk, 'label'):\n",
                "            entity = ' '.join(c[0] for c in chunk.leaves())\n",
                "            entity_type = chunk.label()\n",
                "            entities.append(f\"{entity} ({entity_type})\")\n",
                "    \n",
                "    return entities if entities else ['No named entities found']\n",
                "\n",
                "test_cases = [\n",
                "    [\"John works at Microsoft in Seattle.\"],\n",
                "    [\"The United States and Canada signed a trade agreement.\"],\n",
                "    [\"Tesla CEO Elon Musk announced new plans.\"],\n",
                "    [\"Mount Everest is in Nepal.\"],\n",
                "    [\"Sarah visited Paris last summer.\"]\n",
                "]\n",
                "\n",
                "run_tests(named_entity_recognition, test_cases)\n",
                "# Ignore dowload error in VS Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "tokenize_text",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['Natural language processing helps computers understand and work with human language. '] -> {'words': ['Natural', 'language', 'processing', 'helps', 'computers', 'understand', 'and', 'work', 'with', 'human', 'language', '.'], 'sentences': ['Natural language processing helps computers understand and work with human language.']} | Excel: =TOKENIZE_TEXT(\"Natural language processing helps computers understand and work with human language. \")\n",
                        "Case 2: [\"The researchers are developing better models. They're making progress daily.\"] -> {'words': ['The', 'researchers', 'are', 'developing', 'better', 'models', '.', 'They', \"'re\", 'making', 'progress', 'daily', '.'], 'sentences': ['The researchers are developing better models.', \"They're making progress daily.\"]} | Excel: =TOKENIZE_TEXT(\"The researchers are developing better models. They're making progress daily.\")\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "def tokenize_text(text):\n",
                "    \"\"\"Split text into words and sentences.\n",
                "    Args:\n",
                "        text (str): Input text\n",
                "    Returns:\n",
                "        dict: Dictionary with word and sentence tokens\n",
                "    \"\"\"\n",
                "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
                "    import nltk\n",
                "    nltk.download('punkt')\n",
                "    \n",
                "    return {\n",
                "        'words': word_tokenize(text),\n",
                "        'sentences': sent_tokenize(text)\n",
                "    }\n",
                "\n",
                "test_cases = [\n",
                "    [\"Natural language processing helps computers understand and work with human language.\"],\n",
                "    [\"The researchers are developing better models. They're making progress daily.\"]\n",
                "]\n",
                "\n",
                "run_tests(tokenize_text, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "reduce_words",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: [\"The researchers are developing better models. They're making progress daily.\"] -> {'stemmed': ['the', 'research', 'are', 'develop', 'better', 'model', '.', 'they', \"'re\", 'make', 'progress', 'daili', '.'], 'lemmatized': ['The', 'researcher', 'are', 'developing', 'better', 'model', '.', 'They', \"'re\", 'making', 'progress', 'daily', '.']} | Excel: =REDUCE_WORDS(\"The researchers are developing better models. They're making progress daily.\")\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "def reduce_words(text):\n",
                "    \"\"\"Perform stemming and lemmatization on text.\n",
                "    Args:\n",
                "        text (str): Input text\n",
                "    Returns:\n",
                "        dict: Dictionary with stemmed and lemmatized words\n",
                "    \"\"\"\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
                "    import nltk\n",
                "    nltk.download('punkt')\n",
                "    nltk.download('wordnet')\n",
                "    \n",
                "    words = word_tokenize(text)\n",
                "    stemmer = PorterStemmer()\n",
                "    lemmatizer = WordNetLemmatizer()\n",
                "    return {\n",
                "        'stemmed': [stemmer.stem(word) for word in words],\n",
                "        'lemmatized': [lemmatizer.lemmatize(word) for word in words]\n",
                "    }\n",
                "\n",
                "test_cases = [\n",
                "    [\"The researchers are developing better models. They're making progress daily.\"]\n",
                "]\n",
                "\n",
                "run_tests(reduce_words, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "analyze_pos",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['The quick brown fox jumps over the lazy dog.'] -> [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')] | Excel: =ANALYZE_POS(\"The quick brown fox jumps over the lazy dog.\")\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n"
                    ]
                }
            ],
            "source": [
                "def analyze_pos(text):\n",
                "    \"\"\"Tag parts of speech in text.\n",
                "    Args:\n",
                "        text (str): Input text\n",
                "    Returns:\n",
                "        list: List of (word, POS) tuples\n",
                "    \"\"\"\n",
                "    from nltk import pos_tag, word_tokenize\n",
                "    import nltk\n",
                "    nltk.download('punkt')\n",
                "    nltk.download('averaged_perceptron_tagger')\n",
                "    \n",
                "    words = word_tokenize(text)\n",
                "    return pos_tag(words)\n",
                "\n",
                "test_cases = [\n",
                "    [\"The quick brown fox jumps over the lazy dog.\"]\n",
                "]\n",
                "\n",
                "run_tests(analyze_pos, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "process_text",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['Natural language processing helps computers understand and work with human language!'] -> ['natural', 'language', 'processing', 'helps', 'computers', 'understand', 'work', 'human', 'language'] | Excel: =PROCESS_TEXT(\"Natural language processing helps computers understand and work with human language!\")\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "def process_text(text):\n",
                "    \"\"\"Clean and process text.\n",
                "    Args:\n",
                "        text (str): Input text\n",
                "    Returns:\n",
                "        list: Processed words\n",
                "    \"\"\"\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    from nltk.corpus import stopwords\n",
                "    import nltk\n",
                "    nltk.download('punkt')\n",
                "    nltk.download('stopwords')\n",
                "    \n",
                "    stop_words = set(stopwords.words('english'))\n",
                "    words = word_tokenize(text.lower())\n",
                "    return [word for word in words if word.isalnum() and word not in stop_words]\n",
                "\n",
                "test_cases = [\n",
                "    [\"Natural language processing helps computers understand and work with human language!\"]\n",
                "]\n",
                "\n",
                "run_tests(process_text, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "find_collocations",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['Natural language processing helps computers understand human language.'] -> {'bigrams': [('Natural', 'language'), ('language', 'processing'), ('processing', 'helps'), ('helps', 'computers'), ('computers', 'understand'), ('understand', 'human'), ('human', 'language'), ('language', '.')], 'trigrams': [('Natural', 'language', 'processing'), ('language', 'processing', 'helps'), ('processing', 'helps', 'computers'), ('helps', 'computers', 'understand'), ('computers', 'understand', 'human'), ('understand', 'human', 'language'), ('human', 'language', '.')]} | Excel: =FIND_COLLOCATIONS(\"Natural language processing helps computers understand human language.\")\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "def find_collocations(text):\n",
                "    \"\"\"Find common word pairs and trigrams.\n",
                "    Args:\n",
                "        text (str): Input text\n",
                "    Returns:\n",
                "        dict: Dictionary with bigrams and trigrams\n",
                "    \"\"\"\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    from nltk import ngrams\n",
                "    import nltk\n",
                "    nltk.download('punkt')\n",
                "    \n",
                "    words = word_tokenize(text)\n",
                "    return {\n",
                "        'bigrams': list(ngrams(words, 2)),\n",
                "        'trigrams': list(ngrams(words, 3))\n",
                "    }\n",
                "\n",
                "test_cases = [\n",
                "    [\"Natural language processing helps computers understand human language.\"]\n",
                "]\n",
                "\n",
                "run_tests(find_collocations, test_cases)"
            ]
        }
    ],
    "metadata": {
        "boardflare": {
            "author": "Boardflare",
            "description": "NLP using NLTK library",
            "tags": [
                "simple"
            ],
            "title": "NLTK"
        },
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
