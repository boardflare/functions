{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "8384688e",
            "metadata": {
                "tags": [
                    "Runner"
                ]
            },
            "outputs": [],
            "source": [
                "def run_tests(func, test_cases):\n",
                "    for i, args in enumerate(test_cases):\n",
                "        result = func(*args)\n",
                "        print(f\"Case {i+1}: {args} -> {result}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "f273cdc0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['I love this product!'] -> 0.6696\n",
                        "Case 2: ['This is terrible.'] -> -0.4767\n",
                        "Case 3: ['The weather is nice today.'] -> 0.4215\n",
                        "Case 4: ['I am so angry right now!'] -> -0.5974\n",
                        "Case 5: ['This seems neutral.'] -> 0.0\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package vader_lexicon to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "from nltk.sentiment import SentimentIntensityAnalyzer\n",
                "\n",
                "# Ensure you have downloaded the necessary NLTK data\n",
                "nltk.download('vader_lexicon')\n",
                "\n",
                "def sentiment_analysis(text):\n",
                "    \"\"\"Analyzes sentiment of text.\n",
                "    Args:\n",
                "        text (str): Text to analyze\n",
                "    Returns:\n",
                "        float: Compound sentiment score (-1 to 1)\n",
                "    \"\"\"\n",
                "    sia = SentimentIntensityAnalyzer()\n",
                "    sentiment = sia.polarity_scores(text)\n",
                "    return sentiment['compound']\n",
                "\n",
                "test_cases = [\n",
                "    [\"I love this product!\"],           # -> positive score\n",
                "    [\"This is terrible.\"],              # -> negative score\n",
                "    [\"The weather is nice today.\"],     # -> slightly positive\n",
                "    [\"I am so angry right now!\"],       # -> negative score\n",
                "    [\"This seems neutral.\"]             # -> neutral score\n",
                "]\n",
                "\n",
                "# Excel usage: =SENTIMENT_ANALYSIS(\"I love this product!\")\n",
                "\n",
                "run_tests(sentiment_analysis, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "intent_classification_improved",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: [\"What's the temperature today?\"] -> Question\n",
                        "Case 2: ['Can you help me find my files?'] -> Request\n",
                        "Case 3: ['Hi, nice to meet you!'] -> Greeting\n",
                        "Case 4: ['Bye, thanks for all your help!'] -> Farewell\n",
                        "Case 5: ['The system is working perfectly.'] -> Question\n",
                        "Case 6: ['Where did you put the documents?'] -> Question\n",
                        "Case 7: ['Please show me the way.'] -> Request\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "def classify_intent(text):\n",
                "    \"\"\"Classify the intent of the input text using NLTK's NaiveBayesClassifier.\n",
                "    Args:\n",
                "        text (str): Text to analyze\n",
                "    Returns:\n",
                "        str: Classified intent (capitalized)\n",
                "    \"\"\"\n",
                "    import nltk\n",
                "    from nltk.classify import NaiveBayesClassifier\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    nltk.download('punkt')\n",
                "    \n",
                "    def extract_features(text):\n",
                "        \"\"\"Extract features from text for classification.\"\"\"\n",
                "        words = word_tokenize(text.lower())\n",
                "        return dict([(word, True) for word in words])\n",
                "    \n",
                "    # Training data with labeled intents\n",
                "    training_data = [\n",
                "        (\"what is the weather like\", \"question\"),\n",
                "        (\"what time is it\", \"question\"),\n",
                "        (\"where are you from\", \"question\"),\n",
                "        (\"who made this\", \"question\"),\n",
                "        (\"please help me\", \"request\"),\n",
                "        (\"could you assist me\", \"request\"),\n",
                "        (\"show me how to\", \"request\"),\n",
                "        (\"i need help with\", \"request\"),\n",
                "        (\"hello there\", \"greeting\"),\n",
                "        (\"hi how are you\", \"greeting\"),\n",
                "        (\"good morning\", \"greeting\"),\n",
                "        (\"nice to meet you\", \"greeting\"),\n",
                "        (\"goodbye for now\", \"farewell\"),\n",
                "        (\"see you later\", \"farewell\"),\n",
                "        (\"i have to go\", \"farewell\"),\n",
                "        (\"thanks for your help\", \"farewell\"),\n",
                "        (\"i like this product\", \"statement\"),\n",
                "        (\"the weather is nice\", \"statement\"),\n",
                "        (\"this works well\", \"statement\"),\n",
                "        (\"interesting idea\", \"statement\")\n",
                "    ]\n",
                "    \n",
                "    # Prepare and train the classifier\n",
                "    featuresets = [(extract_features(text), intent) for (text, intent) in training_data]\n",
                "    classifier = NaiveBayesClassifier.train(featuresets)\n",
                "    \n",
                "    # Classify the input text\n",
                "    features = extract_features(text)\n",
                "    intent = classifier.classify(features)\n",
                "    return intent.capitalize()\n",
                "\n",
                "test_cases = [\n",
                "    [\"What's the temperature today?\"],\n",
                "    [\"Can you help me find my files?\"],\n",
                "    [\"Hi, nice to meet you!\"],\n",
                "    [\"Bye, thanks for all your help!\"],\n",
                "    [\"The system is working perfectly.\"],\n",
                "    [\"Where did you put the documents?\"],\n",
                "    [\"Please show me the way.\"]\n",
                "]\n",
                "\n",
                "# Excel usage: =CLASSIFY_INTENT(\"What's the temperature today?\")\n",
                "\n",
                "run_tests(classify_intent, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "9e8116cf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package maxent_ne_chunker to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
                        "[nltk_data] Downloading package words to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package words is already up-to-date!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['John works at Microsoft in Seattle.'] -> ['John (PERSON)', 'Microsoft (ORGANIZATION)', 'Seattle (GPE)']\n",
                        "Case 2: ['The United States and Canada signed a trade agreement.'] -> ['United States (GPE)', 'Canada (GPE)']\n",
                        "Case 3: ['Tesla CEO Elon Musk announced new plans.'] -> ['Tesla (PERSON)', 'CEO Elon Musk (ORGANIZATION)']\n",
                        "Case 4: ['Mount Everest is in Nepal.'] -> ['Mount (PERSON)', 'Everest (ORGANIZATION)', 'Nepal (GPE)']\n",
                        "Case 5: ['Sarah visited Paris last summer.'] -> ['Sarah (PERSON)', 'Paris (GPE)']\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "import numpy\n",
                "\n",
                "# Download all required NLTK data\n",
                "for package in ['punkt', 'averaged_perceptron_tagger_eng', 'maxent_ne_chunker', 'words']:\n",
                "    nltk.download(package)\n",
                "\n",
                "def named_entity_recognition(text):\n",
                "    \"\"\"Extract named entities from text.\n",
                "    Args:\n",
                "        text (str): Text to analyze\n",
                "    Returns:\n",
                "        list: List of named entities found\n",
                "    \"\"\"\n",
                "    # Tokenize and tag the text\n",
                "    tokens = nltk.word_tokenize(text)\n",
                "    pos_tags = nltk.pos_tag(tokens)\n",
                "    \n",
                "    # Extract named entities\n",
                "    named_entities = nltk.ne_chunk(pos_tags)\n",
                "    entities = []\n",
                "    \n",
                "    # Process each chunk\n",
                "    for chunk in named_entities:\n",
                "        if hasattr(chunk, 'label'):\n",
                "            entity = ' '.join(c[0] for c in chunk.leaves())\n",
                "            entity_type = chunk.label()\n",
                "            entities.append(f\"{entity} ({entity_type})\")\n",
                "    \n",
                "    return entities if entities else ['No named entities found']\n",
                "\n",
                "test_cases = [\n",
                "    [\"John works at Microsoft in Seattle.\"],\n",
                "    [\"The United States and Canada signed a trade agreement.\"],\n",
                "    [\"Tesla CEO Elon Musk announced new plans.\"],\n",
                "    [\"Mount Everest is in Nepal.\"],\n",
                "    [\"Sarah visited Paris last summer.\"]\n",
                "]\n",
                "\n",
                "# Excel usage: =NAMED_ENTITY_RECOGNITION(\"John works at Microsoft in Seattle.\")\n",
                "\n",
                "run_tests(named_entity_recognition, test_cases)\n",
                "# Ignore dowload error in VS Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "tokenize_text",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['Natural language processing helps computers understand and work with human language.'] -> ['Natural', 'language', 'processing', 'helps', 'computers', 'understand', 'and', 'work', 'with', 'human', 'language', '.']\n",
                        "Case 2: [\"The researchers are developing better models. They're making progress daily.\"] -> ['The', 'researchers', 'are', 'developing', 'better', 'models', '.', 'They', \"'re\", 'making', 'progress', 'daily', '.']\n"
                    ]
                }
            ],
            "source": [
                "def split_into_words(text):\n",
                "    \"\"\"Tokenize the given text into words.\n",
                "    Args:\n",
                "        text (str): Input text to tokenize\n",
                "    Returns:\n",
                "        list: List of words\n",
                "    \"\"\"\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    \n",
                "    return word_tokenize(text)\n",
                "\n",
                "test_cases = [\n",
                "    [\"Natural language processing helps computers understand and work with human language.\"],\n",
                "    [\"The researchers are developing better models. They're making progress daily.\"]\n",
                "]\n",
                "\n",
                "# Excel usage: =SPLIT_INTO_WORDS(\"Natural language processing helps computers understand and work with human language.\")\n",
                "\n",
                "run_tests(split_into_words, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "reduce_words",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: [\"The researchers are developing better models. They're making progress daily.\"] -> ['the', 'research', 'are', 'develop', 'better', 'model', '.', 'they', \"'re\", 'make', 'progress', 'daili', '.']\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "def stem_words(text):\n",
                "    \"\"\"Perform stemming on text.\n",
                "    Args:\n",
                "        text (str): Input text\n",
                "    Returns:\n",
                "        list: List of stemmed words\n",
                "    \"\"\"\n",
                "    from nltk.tokenize import word_tokenize\n",
                "    from nltk.stem import PorterStemmer\n",
                "    import nltk\n",
                "    nltk.download('punkt')\n",
                "    \n",
                "    words = word_tokenize(text)\n",
                "    stemmer = PorterStemmer()\n",
                "    return [stemmer.stem(word) for word in words]\n",
                "\n",
                "test_cases = [\n",
                "    [\"The researchers are developing better models. They're making progress daily.\"]  \n",
                "    # -> ['the', 'research', 'are', 'develop', 'better', 'model', '.', 'they', \"'re\", 'make', 'progress', 'daili', '.']\n",
                "]\n",
                "\n",
                "# Excel usage: =STEM_WORDS(\"The researchers are developing better models. They're making progress daily.\")\n",
                "\n",
                "run_tests(stem_words, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "analyze_pos",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['The quick brown fox jumps over the lazy dog.'] -> ['The:DT', 'quick:JJ', 'brown:NN', 'fox:NN', 'jumps:VBZ', 'over:IN', 'the:DT', 'lazy:JJ', 'dog:NN', '.:.']\n",
                        "Case 2: ['I am running!'] -> ['I:PRP', 'am:VBP', 'running:VBG', '!:.']\n",
                        "Case 3: ['Python is great.'] -> ['Python:NNP', 'is:VBZ', 'great:JJ', '.:.']\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     C:\\Users\\brent\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n"
                    ]
                }
            ],
            "source": [
                "def analyze_pos(text):\n",
                "    \"\"\"Tag parts of speech in text.\n",
                "    Args:\n",
                "        text (str): Input text\n",
                "    Returns:\n",
                "        list: List of strings in 'word:POS' format\n",
                "    \"\"\"\n",
                "    from nltk import pos_tag, word_tokenize\n",
                "    import nltk\n",
                "    nltk.download('punkt')\n",
                "    nltk.download('averaged_perceptron_tagger')\n",
                "    \n",
                "    words = word_tokenize(text)\n",
                "    pos_tags = pos_tag(words)\n",
                "    return [f\"{word}:{pos}\" for word, pos in pos_tags]\n",
                "\n",
                "test_cases = [\n",
                "    [\"The quick brown fox jumps over the lazy dog.\"],  \n",
                "    # -> ['The:DT', 'quick:JJ', 'brown:JJ', 'fox:NN', 'jumps:VBZ', 'over:IN', 'the:DT', 'lazy:JJ', 'dog:NN', '.:.',]\n",
                "    [\"I am running!\"],\n",
                "    # -> ['I:PRP', 'am:VBP', 'running:VBG', '!:.']\n",
                "    [\"Python is great.\"]\n",
                "    # -> ['Python:NNP', 'is:VBZ', 'great:JJ', '.:.']\n",
                "]\n",
                "\n",
                "# Excel usage: =ANALYZE_POS(\"The quick brown fox jumps over the lazy dog.\")\n",
                "\n",
                "run_tests(analyze_pos, test_cases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "65f3802d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Case 1: ['hello',              0\n",
                        "0  hello world\n",
                        "1     hi there\n",
                        "2    greetings, 'levenshtein'] -> [[1, 0.45]]\n",
                        "Case 2: ['programing',              0\n",
                        "0  programming\n",
                        "1       coding\n",
                        "2  development, 'levenshtein'] -> [[1, 0.91]]\n",
                        "Case 3: ['python',             0\n",
                        "0     python3\n",
                        "1  javascript\n",
                        "2        java, 'jaccard'] -> [[1, 0.83]]\n",
                        "Case 4: ['artificial intelligence',                   0\n",
                        "0  machine learning\n",
                        "1  artificial intel\n",
                        "2                AI, 'jaro'] -> [[2, 0.9]]\n",
                        "Case 5: ['data science',                 0\n",
                        "0   data analysis\n",
                        "1  data scientist\n",
                        "2      statistics, 'jaccard'] -> [[2, 0.6]]\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "from nltk.metrics.distance import edit_distance, jaccard_distance, jaro_similarity\n",
                "from nltk.util import ngrams\n",
                "\n",
                "def text_similarity(lookup_value, lookup_array_df, algorithm):\n",
                "    \"\"\"\n",
                "    Calculate the similarity between a lookup_value and the best match in a lookup_array.\n",
                "\n",
                "    Parameters:\n",
                "    lookup_value (str or pd.DataFrame): The string or DataFrame to search for.\n",
                "    lookup_array_df (pd.DataFrame): The DataFrame to search within.\n",
                "    algorithm (str): The algorithm to use for calculating similarity. Options are 'levenshtein', 'jaccard', and 'jaro'. Default is 'jaccard'.\n",
                "\n",
                "    Returns:\n",
                "    list: A list of lists where each sublist contains the index (1-based) and the similarity score of the most similar item in the lookup_array.\n",
                "    \"\"\"\n",
                "    # Define a dictionary to map algorithm names to functions\n",
                "    algo_funcs = {\n",
                "        'levenshtein': lambda x, y: 1 - edit_distance(x, y) / max(len(x), len(y)),\n",
                "        'jaccard': lambda x, y: 1 - jaccard_distance(set(ngrams(x, 2)), set(ngrams(y, 2))),\n",
                "        'jaro': jaro_similarity\n",
                "    }\n",
                "    \n",
                "    # Get the algorithm function from the dictionary\n",
                "    algo_func = algo_funcs.get(algorithm)\n",
                "    if algo_func is None:\n",
                "        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n",
                "    \n",
                "    # Flatten the DataFrame to a list\n",
                "    lookup_array = lookup_array_df.values.flatten().tolist()\n",
                "    \n",
                "    # Check if lookup_value is a DataFrame\n",
                "    if isinstance(lookup_value, pd.DataFrame):\n",
                "        lookup_value_list = lookup_value.values.flatten().tolist()\n",
                "    else:\n",
                "        lookup_value_list = [lookup_value]\n",
                "    \n",
                "    results = [] \n",
                "    for lookup_value_item in lookup_value_list:\n",
                "        # Calculate similarity scores and round to 2 decimal places\n",
                "        scores = [(index + 1, round(algo_func(lookup_value_item, item), 2)) for index, item in enumerate(lookup_array)]\n",
                "        \n",
                "        # Sort based on scores in descending order\n",
                "        scores.sort(key=lambda x: x[1], reverse=True)\n",
                "        # Append the top index and score to results as a list\n",
                "        results.append(list(scores[0]))\n",
                "\n",
                "    # results is 2D list, e.g. [[1, 0.75], [2, 0.85]]\n",
                "    return results\n",
                "\n",
                "test_cases = [\n",
                "    [\"hello\", pd.DataFrame([\"hello world\", \"hi there\", \"greetings\"]), \"levenshtein\"],  # -> [1, 0.7]\n",
                "    [\"programing\", pd.DataFrame([\"programming\", \"coding\", \"development\"]), \"levenshtein\"],  # -> [1, 0.9]\n",
                "    [\"python\", pd.DataFrame([\"python3\", \"javascript\", \"java\"]), \"jaccard\"],  # -> [1, 0.67]\n",
                "    [\"artificial intelligence\", pd.DataFrame([\"machine learning\", \"artificial intel\", \"AI\"]), \"jaro\"],  # -> [2, 0.85]\n",
                "    [\"data science\", pd.DataFrame([\"data analysis\", \"data scientist\", \"statistics\"]), \"jaccard\"]  # -> [1, 0.6]\n",
                "]\n",
                "\n",
                "# Excel usage: =TEXT_SIMILARITY(\"hello\", {\"hello world\", \"hi there\", \"greetings\"}, \"levenshtein\")\n",
                "\n",
                "run_tests(text_similarity, test_cases)"
            ]
        }
    ],
    "metadata": {
        "boardflare": {
            "author": "Boardflare",
            "description": "NLP using NLTK package",
            "tags": [
                "simple"
            ],
            "title": "NLTK examples"
        },
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
